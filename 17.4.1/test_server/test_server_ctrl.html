<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>test_server_ctrl (test_server) -  (Erlang Documentation)</title>
    <link href="../erldocs.css" type="text/css" rel="stylesheet"/>
    <link href="/search.xml" rel="search" type="application/opensearchdescription+xml" title="erldocs"/>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-44246018-1', 'auto');
      ga('send', 'pageview');
    </script>
  </head>

  <body>
    <div id="sidebar" class="inactive">
      <input type="text" id="search" autocomplete="off" placeholder="press TAB to search"/>
      <ul id="results"> </ul>
    </div>

    <div id="content">
      <div style="margin:0px; padding:10px 20px;">
        
  
  <h1>test_server_ctrl</h1>
  <h2 class="modsummary">This module provides a low level interface to the Test Server.</h2>
  <div class="description">
    <p>The <code>test_server_ctrl</code> module provides a low level
      interface to the Test Server. This interface is normally
      not used directly by the tester, but through a framework built 
      on top of <code>test_server_ctrl</code>.
      </p>
    <p>Common Test is such a framework, well suited for automated
      black box testing of target systems of any kind (not necessarily
      implemented in Erlang). Common Test is also a very useful tool for
      white box testing Erlang programs and OTP applications. 
      Please see the Common Test User's Guide and reference manual for 
      more information.
      </p>
    <p>If you want to write your own framework, some more information
      can be found in the chapter "Writing your own test server
      framework" in the Test Server User's Guide. Details about the
      interface provided by <code>test_server_ctrl</code> follows below.
      </p>
  </div>
  <div id="functions" class="category"><h4><a href="#functions">Functions</a></h4><hr />
    <div class="function">
      <h3 id="start/0">start() -&gt; Result</h3>
      
      <ul class="type">
        <li><code>Result = ok | {error, {already_started, pid()}</code></li>
      </ul>
      <div class="description">
        <p>This function starts the test server.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="stop/0">stop() -&gt; ok</h3>
      
      <div class="description">
        <p>This stops the test server and
          all its activity.  The running test suite (if any) will be
          halted.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="add_dir/2">add_dir(Name, Dir) -&gt; ok</h3>
      <h3 id="add_dir/3">add_dir(Name, Dir, Pattern) -&gt; ok</h3>
      <h3 id="add_dir/2-1">add_dir(Name, [Dir|Dirs]) -&gt; ok</h3>
      <h3 id="add_dir/3-1">add_dir(Name, [Dir|Dirs], Pattern) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Name = term()</code></li>
        <d>The jobname for this directory.</d>
        <li><code>Dir = term()</code></li>
        <d>The directory to scan for test suites.</d>
        <li><code>Dirs = [term()]</code></li>
        <d>List of directories to scan for test suites.</d>
        <li><code>Pattern = term()</code></li>
        <d>Suite match pattern. Directories will be scanned for Pattern_SUITE.erl files.</d>
      </ul>
      <div class="description">
        <p>Puts a collection of suites matching (*_SUITE) in given
          directories into the job queue. <code>Name</code> is an arbitrary
          name for the job, it can be any erlang term. If <code>Pattern</code>
          is given, only modules matching <code>Pattern*</code> will be added.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="add_module/1">add_module(Mod) -&gt; ok</h3>
      <h3 id="add_module/2">add_module(Name, [Mod|Mods]) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Mod = atom()</code></li>
        <li><code>Mods = [atom()]</code></li>
        <d>The name(s) of the module(s) to add.</d>
        <li><code>Name = term()</code></li>
        <d>Name for the job.</d>
      </ul>
      <div class="description">
        <p>This function adds a module or a list of modules, to the
          test servers job queue. <code>Name</code> may be any Erlang
          term. When <code>Name</code> is not given, the job gets the name of
          the module.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="add_case/2">add_case(Mod, Case) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Mod = atom()</code></li>
        <d>Name of the module the test case is in.</d>
        <li><code>Case = atom() </code></li>
        <d>Function name of the test case to add.</d>
      </ul>
      <div class="description">
        <p>This function will add one test case to the job queue. The
          job will be given the module's name.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="add_case/3">add_case(Name, Mod, Case) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Name = string()</code></li>
        <d>Name to use for the test job.</d>
      </ul>
      <div class="description">
        <p>Equivalent to <code>add_case/2</code>, but the test job will get
          the specified name.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="add_cases/2">add_cases(Mod, Cases) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Mod = atom()</code></li>
        <d>Name of the module the test case is in.</d>
        <li><code>Cases = [Case] </code></li>
        <li><code>Case = atom() </code></li>
        <d>Function names of the test cases to add.</d>
      </ul>
      <div class="description">
        <p>This function will add one or more test cases to the job
          queue. The job will be given the module's name.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="add_cases/3">add_cases(Name, Mod, Cases) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Name = string()</code></li>
        <d>Name to use for the test job.</d>
      </ul>
      <div class="description">
        <p>Equivalent to <code>add_cases/2</code>, but the test job will get
          the specified name.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="add_spec/1">add_spec(TestSpecFile) -&gt; ok | {error, nofile}</h3>
      
      <ul class="type">
        <li><code>TestSpecFile = string()</code></li>
        <d>Name of the test specification file</d>
      </ul>
      <div class="description">
        <p>This function will add the content of the given test
          specification file to the job queue. The job will be given the
          name of the test specification file, e.g. if the file is
          called <code>test.spec</code>, the job will be called <code>test</code>.
          </p>
        <p>See the reference manual for the test server application
          for details about the test specification file.</p>
      </div>
      </div>
      <div class="function">
        <h3 id="add_dir_with_skip/3">add_dir_with_skip(Name, [Dir|Dirs], Skip) -&gt; ok</h3>
	<h3 id="add_dir_with_skip/4">add_dir_with_skip(Name, [Dir|Dirs], Pattern, Skip) -&gt; ok</h3>
        <h3 id="add_module_with_skip/2">add_module_with_skip(Mod, Skip) -&gt; ok</h3>
        <h3 id="add_module_with_skip/3">add_module_with_skip(Name, [Mod|Mods], Skip) -&gt; ok</h3>
        <h3 id="add_case_with_skip/3">add_case_with_skip(Mod, Case, Skip) -&gt; ok</h3>
        <h3 id="add_case_with_skip/4">add_case_with_skip(Name, Mod, Case, Skip) -&gt; ok</h3>
	<h3 id="add_cases_with_skip/3">add_cases_with_skip(Mod, Cases, Skip) -&gt; ok</h3>
	<h3 id="add_cases_with_skip/4">add_cases_with_skip(Name, Mod, Cases, Skip) -&gt; ok</h3>
	
	<ul class="type">
	<li><code>Skip = [SkipItem]</code></li>
	<d>List of items to be skipped from the test.</d>
        <li><code>SkipItem = {Mod,Comment} | {Mod,Case,Comment} | {Mod,Cases,Comment}</code></li>
	<li><code>Mod = atom()</code></li>
	<d>Test suite name.</d>
	<li><code>Comment = string()</code></li>
	<d>Reason why suite or case is being skipped.</d>
	<li><code>Cases = [Case]</code></li>
	<li><code>Case = atom()</code></li>
	<d>Name of test case function.</d>
	</ul>
	<div class="description">
	  <p>These functions add test jobs just like the add_dir, add_module,
	    add_case and add_cases functions above, but carry an additional
	    argument, Skip. Skip is a list of items that should be skipped
	    in the current test run. Test job items that occur in the Skip
	    list will be logged as SKIPPED with the associated Comment.</p>
	</div>
    </div>
    <div class="function">
      <h3 id="add_tests_with_skip/3">add_tests_with_skip(Name, Tests, Skip) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Name = term()</code></li>
        <d>The jobname for this directory.</d>
	<li><code>Tests = [TestItem]</code></li>
	<d>List of jobs to add to the run queue.</d>
	<li><code>TestItem = {Dir,all,all} | {Dir,Mods,all} | {Dir,Mod,Cases}</code></li>
        <li><code>Dir = term()</code></li>
        <d>The directory to scan for test suites.</d>
	<li><code>Mods = [Mod]</code></li>
	<li><code>Mod = atom()</code></li>
	<d>Test suite name.</d>
	<li><code>Cases = [Case]</code></li>
	<li><code>Case = atom()</code></li>
	<d>Name of test case function.</d>
	<li><code>Skip = [SkipItem]</code></li>
	<d>List of items to be skipped from the test.</d>
        <li><code>SkipItem = {Mod,Comment} | {Mod,Case,Comment} | {Mod,Cases,Comment}</code></li>
	<li><code>Comment = string()</code></li>
	<d>Reason why suite or case is being skipped.</d>
      </ul>
	<div class="description">
	<p>This function adds various test jobs to the test_server_ctrl
	job queue. These jobs can be of different type (all or specific suites 
	in one directory, all or specific cases in one suite, etc). It is also
	possible to get particular items skipped by passing them along in the
	Skip list (see the add_*_with_skip functions above).</p>
	</div>
    </div>
    <div class="function">
      <h3 id="abort_current_testcase/1">abort_current_testcase(Reason) -&gt; ok | {error,no_testcase_running}</h3>
      
      <ul class="type">
      <li><code>Reason = term()</code></li>
      <d>The reason for stopping the test case, which will be printed in the log.</d>
      </ul>
	<div class="description">
	<p>When calling this function, the currently executing test case will be aborted.
	   It is the user's responsibility to know for sure which test case is currently
	   executing. The function is therefore only safe to call from a function which
	   has been called (or synchronously invoked) by the test case.</p>
	</div>
    </div>
    <div class="function">
      <h3 id="set_levels/3">set_levels(Console, Major, Minor) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Console = integer()</code></li>
        <d>Level for I/O to be sent to console.</d>
        <li><code>Major = integer()</code></li>
        <d>Level for I/O to be sent to the major logfile.</d>
        <li><code>Minor = integer()</code></li>
        <d>Level for I/O to be sent to the minor logfile.</d>
      </ul>
      <div class="description">
        <p>Determines where I/O from test suites/test server will
          go. All text output from test suites and the test server is
          tagged with a priority value which ranges from 0 to 100, 100
          being the most detailed. (see the section about log files in
          the user's guide). Output from the test cases (using
          <code>io:format/2</code>) has a detail level of 50. Depending on the
          levels set by this function, this I/O may be sent to the
          console, the major log file (for the whole test suite) or to
          the minor logfile (separate for each test case).
          </p>
        <p>All output with detail level:</p>
        <list type="bulleted">
          <item>Less than or equal to <code>Console</code> is displayed on
           the screen (default 1)
          </item>
          <item>Less than or equal to <code>Major</code> is logged in the
           major log file (default 19)
          </item>
          <item>Greater than or equal to <code>Minor</code> is logged in the
           minor log files (default 10)
          </item>
        </list>
        <p>To view the currently set thresholds, use the
          <code>get_levels/0</code> function.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="get_levels/0">get_levels() -&gt; {Console, Major, Minor}</h3>
      
      <div class="description">
        <p>Returns the current levels. See <code>set_levels/3</code> for
          types.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="jobs/0">jobs() -&gt; JobQueue</h3>
      
      <ul class="type">
        <li><code>JobQueue = [{list(), pid()}]</code></li>
      </ul>
      <div class="description">
        <p>This function will return all the jobs currently in the job
          queue.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="multiply_timetraps/1">multiply_timetraps(N) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>N = integer() | infinity</code></li>
      </ul>
      <div class="description">
        <p>This function should be called before a test is started
          which requires extended timetraps, e.g. if extensive tracing
          is used. All timetraps started after this call will be
          multiplied by <code>N</code>.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="scale_timetraps/1">scale_timetraps(Bool) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Bool = true | false</code></li>
      </ul>
      <div class="description">
        <p>This function should be called before a test is started.
	  The parameter specifies if test_server should attempt
	  to automatically scale the timetrap value in order to compensate
	  for delays caused by e.g. the cover tool.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="get_timetrap_parameters/0">get_timetrap_parameters() -&gt; {N,Bool} </h3>
      
      <ul class="type">
        <li><code>N = integer() | infinity</code></li>
        <li><code>Bool = true | false</code></li>
      </ul>
      <div class="description">
        <p>This function may be called to read the values set by
	<code>multiply_timetraps/1</code> and <code>scale_timetraps/1</code>.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="cover/2">cover(Application,Analyse) -&gt; ok</h3>
      <h3 id="cover/2-1">cover(CoverFile,Analyse) -&gt; ok</h3>
      <h3 id="cover/3">cover(App,CoverFile,Analyse) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Application = atom()</code></li>
        <d>OTP application to cover compile</d>
        <li><code>CoverFile = string()</code></li>
        <d>Name of file listing modules to exclude from or include in cover compilation. The filename must include full path to the file.</d>
        <li><code>Analyse = details | overview</code></li>
      </ul>
      <div class="description">
        <p>This function informs the test_server controller that next
          test shall run with code coverage analysis. All timetraps will
          automatically be multiplied by 10 when cover i run.
          </p>
        <p><code>Application</code> and <code>CoverFile</code> indicates what to
          cover compile. If <code>Application</code> is given, the default is
          that all modules in the <code>ebin</code> directory of the
          application will be cover compiled. The <code>ebin</code> directory
          is found by adding <code>ebin</code> to
          <code>code:lib_dir(Application)</code>.
          </p>
        <p>A <code>CoverFile</code> can have the following entries:</p>
        <pre class="sh_erlang">
{exclude, all | ExcludeModuleList}.
{include, IncludeModuleList}.
{cross, CrossCoverInfo}.</pre>
        <p>Note that each line must end with a full
          stop. <code>ExcludeModuleList</code> and <code>IncludeModuleList</code>
          are lists of atoms, where each atom is a module name.
          </p>

	<p><code>CrossCoverInfo</code> is used when collecting cover data
	  over multiple tests. Modules listed here are compiled, but
	  they will not be analysed when the test is finished. See
	  <a href="#cross_cover_analyse/2" class="seealso">cross_cover_analyse/2</a>
	  for more information about the cross cover mechanism and the
	  format of <code>CrossCoverInfo</code>.
	  </p>
        <p>If both an <code>Application</code> and a <code>CoverFile</code> is
          given, all modules in the application are cover compiled,
          except for the modules listed in <code>ExcludeModuleList</code>. The
          modules in <code>IncludeModuleList</code> are also cover compiled.
          </p>
        <p>If a <code>CoverFile</code> is given, but no <code>Application</code>,
          only the modules in <code>IncludeModuleList</code> are cover
          compiled.
          </p>
        <p><code>Analyse</code> indicates the detail level of the cover
          analysis. If <code>Analyse = details</code>, each cover compiled
          module will be analysed with
          <code>cover:analyse_to_file/1</code>. If <code>Analyse = overview</code>
          an overview of all cover compiled modules is created, listing
          the number of covered and not covered lines for each module.
          </p>
        <p>If the test following this call starts any slave or peer
          nodes with <code>test_server:start_node/3</code>, the same cover
          compiled code will be loaded on all nodes. If the loading
          fails, e.g. if the node runs an old version of OTP, the node
          will simply not be a part of the coverage analysis. Note that
          slave or peer nodes must be stopped with
          <code>test_server:stop_node/1</code> for the node to be part of the
          coverage analysis, else the test server will not be able to
          fetch coverage data from the node.
          </p>
        <p>When the test is finished, the coverage analysis is
          automatically completed, logs are created and the cover
          compiled modules are unloaded. If another test is to be run
          with coverage analysis, <code>test_server_ctrl:cover/2/3</code> must
          be called again.
          </p>
      </div>
    </div>
    <div class="function">
      <h3 id="cross_cover_analyse/2">cross_cover_analyse(Level, Tests) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Level = details | overview</code></li>
        <li><code>Tests = [{Tag,LogDir}]</code></li>
	<li><code>Tag = atom()</code></li>
	<d>Test identifier.</d>
	<li><code>LogDir = string()</code></li>
	<d>Log directory for the test identified by <code>Tag</code>. This
	  can either be the <code>run.&lt;timestamp&gt;</code> directory or
	  the parent directory of this (in which case the latest
	  <code>run.&lt;timestamp&gt;</code> directory is chosen.</d>
      </ul>
      <div class="description">
        <p>Analyse cover data collected from multiple tests. The modules
          analysed are the ones listed in <code>cross</code> statements in
          the cover files. These are modules that are heavily used by
          other tests than the one where they belong or are explicitly
          tested. They should then be listed as cross modules in the
          cover file for the test where they are used but do not
          belong. Se example below.</p>
        <p>This function should be run after all tests are completed,
          and the result will be stored in a file called
          <code>cross_cover.html</code> in the <code>run.&lt;timestamp&gt;</code>
          directory of the test the modules belong to.</p>
	<p>Note that the function can be executed on any node, and it
	  does not require <code>test_server_ctrl</code> to be started first.</p>
        <p>The <code>cross</code> statement in the cover file must be like this:</p>
        <pre class="sh_erlang">
{cross,[{Tag,Modules}]}.</pre>
        <p>where <code>Tag</code> is the same as <code>Tag</code> in the
          <code>Tests</code> parameter to this function and <code>Modules</code> is a
          list of module names (atoms).</p>
	<p><em>Example:</em></p>
	<p>If the module <code>m1</code> belongs to system <code>s1</code> but is
	  heavily used also in the tests for another system <code>s2</code>,
	  then the cover files for the two systems' tests could be like
	  this:</p>
<pre class="sh_erlang">
s1.cover:
  {include,[m1]}.

s2.cover:
  {include,[....]}. % modules belonging to system s2
  {cross,[{s1,[m1]}]}.</pre>
        <p>When the tests for both <code>s1</code> and <code>s2</code> are completed, run</p>
<pre class="sh_erlang">
test_server_ctrl:cross_cover_analyse(Level,[{s1,S1LogDir},{s2,S2LogDir}])
</pre>

        <p>and the accumulated cover data for <code>m1</code> will be written to
	  <code>S1LogDir/[run.&lt;timestamp&gt;/]cross_cover.html</code>.</p>
	<p>Note that the <code>m1</code> module will also be presented in the
	  normal coverage log for <code>s1</code> (due to the include statement in
	  <code>s1.cover</code>), but that only includes the coverage achieved by the
	  <code>s1</code> test itself.</p>
	<p>The Tag in the <code>cross</code> statement in the cover file has
	  no other purpose than mapping the list of modules
	  (<code>[m1]</code> in the example above) to the correct log
	  directory where it should be included in the
	  <code>cross_cover.html</code> file (<code>S1LogDir</code> in the example
	  above). I.e. the value of <code>Tag</code> has no meaning, it
	  could be <code>foo</code> as well as <code>s1</code> above, as long as
	  the same <code>Tag</code> is used in the cover file and in the
	  call to this function.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="trc/1">trc(TraceInfoFile) -&gt; ok | {error, Reason}</h3>
      
      <ul class="type">
        <li><code>TraceInfoFile = atom() | string()</code></li>
        <d>Name of a file defining which functions to trace and how</d>
      </ul>
      <div class="description">
        <p>This function starts call trace on target and on slave or
          peer nodes that are started or will be started by the test
          suites.
          </p>
        <p>Timetraps are not extended automatically when tracing is
          used. Use <code>multiply_timetraps/1</code> if necessary.
          </p>
        <p>Note that the trace support in the test server is in a very
          early stage of the implementation, and thus not yet as
          powerful as one might wish for.
          </p>
        <p>The trace information file specified by the
          <code>TraceInfoFile</code> argument is a text file containing one or
          more of the following elements:
          </p>
        <list type="bulleted">
          <item><code>{SetTP,Module,Pattern}.</code></item>
          <item><code>{SetTP,Module,Function,Pattern}.</code></item>
          <item><code>{SetTP,Module,Function,Arity,Pattern}.</code></item>
          <item><code>ClearTP.</code></item>
          <item><code>{ClearTP,Module}.</code></item>
          <item><code>{ClearTP,Module,Function}.</code></item>
          <item><code>{ClearTP,Module,Function,Arity}.</code></item>
        </list>
        <taglist>
          <dt><code>SetTP = tp | tpl</code></dt>
          <item>This is maps to the corresponding functions in the
          <code>ttb</code> module in the <code>observer</code>
           application. <code>tp</code> means set trace pattern on global
           function calls. <code>tpl</code> means set trace pattern on local
           and global function calls.
          </item>
          <dt><code>ClearTP =  ctp | ctpl | ctpg</code></dt>
          <item>This is maps to the corresponding functions in the
          <code>ttb</code> module in the <code>observer</code>
           application. <code>ctp</code> means clear trace pattern (i.e. turn
           off) on global and local function calls. <code>ctpl</code> means
           clear trace pattern on local function calls only and <code>ctpg</code>
           means clear trace pattern on global function calls only.
          </item>
          <dt><code>Module = atom()</code></dt>
          <item>The module to trace
          </item>
          <dt><code>Function = atom()</code></dt>
          <item>The name of the function to trace
          </item>
          <dt><code>Arity = integer()</code></dt>
          <item>The arity of the function to trace
          </item>
          <dt><code>Pattern = [] | match_spec()</code></dt>
          <item>The trace pattern to set for the module or
           function. For a description of the match_spec() syntax,
           please turn to the User's guide for the runtime system
           (erts). The chapter "Match Specification in Erlang" explains
           the general match specification language.
          </item>
        </taglist>
        <p>The trace result will be logged in a (binary) file called
          <code>NodeName-test_server</code> in the current directory of the
          test server controller node. The log must be formatted using
          <code>ttb:format/1/2</code>.
          </p>
      </div>
    </div>
    <div class="function">
      <h3 id="stop_trace/0">stop_trace() -&gt; ok | {error, not_tracing}</h3>
      
      <div class="description">
        <p>This function stops tracing on target, and on slave or peer
          nodes that are currently running. New slave or peer nodes will
          no longer be traced after this.</p>
      </div>
    </div>
  </div>

  <div class="section">
    <h4>FUNCTIONS INVOKED FROM COMMAND LINE</h4>
    <p>The following functions are supposed to be invoked from the
      command line using the <code>-s</code> option when starting the erlang
      node.</p>
  </div>
  <div id="functions" class="category"><h4><a href="#functions">Functions</a></h4><hr />
    <div class="function">
      <h3 id="run_test/1">run_test(CommandLine) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>CommandLine = FlagList</code></li>
      </ul>
      <div class="description">
        <p>This function is supposed to be invoked from the
          commandline. It starts the test server, interprets the
          argument supplied from the commandline, runs the tests
          specified and when all tests are done, stops the test server
          and returns to the Erlang prompt.
          </p>
        <p>The <code>CommandLine</code> argument is a list of command line
          flags, typically <code>['KEY1', Value1, 'KEY2', Value2, ...]</code>. 
          The valid command line flags are listed below.
          </p>
        <p>Under a UNIX command prompt, this function can be invoked like this:
                    <br />
<code>erl -noshell -s test_server_ctrl run_test KEY1 Value1 KEY2 Value2 ... -s erlang halt</code></p>
        <p>Or make an alias (this is for unix/tcsh)          <br />
<code>alias erl_test 'erl -noshell -s test_server_ctrl run_test \!* -s erlang halt'</code></p>
        <p>And then use it like this          <br />
<code>erl_test KEY1 Value1 KEY2 Value2 ...</code>          <br />
</p>
        <p>The valid command line flags are</p>
        <taglist>
          <dt><code>DIR dir</code></dt>
          <item>Adds all test modules in the directory <code>dir</code> to
           the job queue.
          </item>
          <dt><code>MODULE mod</code></dt>
          <item>Adds the module <code>mod</code> to the job queue.
          </item>
          <dt><code>CASE mod case</code></dt>
          <item>Adds the case <code>case</code> in module <code>mod</code> to the
           job queue.
          </item>
          <dt><code>SPEC spec</code></dt>
          <item>Runs the test specification file <code>spec</code>.
          </item>
          <dt><code>SKIPMOD mod</code></dt>
          <item>Skips all test cases in the module <code>mod</code></item>
          <dt><code>SKIPCASE mod case</code></dt>
          <item>Skips the test case <code>case</code> in module <code>mod</code>.
          </item>
          <dt><code>NAME name</code></dt>
          <item>Names the test suite to something else than the
           default name. This does not apply to <code>SPEC</code> which keeps
           its names.
          </item>
          <dt><code>COVER app cover_file analyse</code></dt>
          <item>Indicates that the test should be run with cover
           analysis. <code>app</code>, <code>cover_file</code> and <code>analyse</code>
           corresponds to the parameters to
          <code>test_server_ctrl:cover/3</code>. If no cover file is used,
           the atom <code>none</code> should be given.
          </item>
          <dt><code>TRACE traceinfofile</code></dt>
          <item>Specifies a trace information file. When this option
           is given, call tracing is started on the target node and all
           slave or peer nodes that are started. The trace information
           file specifies which modules and functions to trace. See the
           function <code>trc/1</code> above for more information about the
           syntax of this file.
          </item>
        </taglist>
      </div>
    </div>
  </div>

  <div class="section">
    <h4>FRAMEWORK CALLBACK FUNCTIONS</h4>
    <p>A test server framework can be defined by setting the
      environment variable <code>TEST_SERVER_FRAMEWORK</code> to a module
      name. This module will then be framework callback module, and it
      must export the following function:</p>
  </div>
  <div id="functions" class="category"><h4><a href="#functions">Functions</a></h4><hr />
    <div class="function">
      <h3 id="get_suite/2">get_suite(Mod,Func) -&gt; TestCaseList</h3>
      
      <ul class="type">
        <li><code>Mod = atom()</code></li>
	<d>Test suite name.</d>
        <li><code>Func = atom()</code></li>
	<d>Name of test case.</d>
        <li><code>TestCaseList = [SubCase]</code></li>
	<d>List of test cases.</d>
        <li><code>SubCase = atom()</code></li>
	<d>Name of a case.</d>
      </ul>
      <div class="description">
        <p>This function is called before a test case is started. The
          purpose is to retrieve a list of subcases. The default
          behaviour of this function should be to call
          <code>Mod:Func(suite)</code> and return the result from this call.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="init_tc/3">init_tc(Mod,Func,Args0) -&gt; {ok,Args1} | {skip,ReasonToSkip} | {auto_skip,ReasonToSkip} | {fail,ReasonToFail}</h3>
      
      <ul class="type">
        <li><code>Mod = atom()</code></li>
	<d>Test suite name.</d>
        <li><code>Func = atom()</code></li>
	<d>Name of test case or configuration function.</d>
        <li><code>Args0 = Args1 = [tuple()]</code></li>
	<d>Normally Args = [Config]</d>
	<li><code>ReasonToSkip = term()</code></li>
	<d>Reason to skip the test case or configuration function.</d>
	<li><code>ReasonToFail = term()</code></li>
	<d>Reason to fail the test case or configuration function.</d>
      </ul>
      <div class="description">
        <p>This function is called before a test case or configuration
	function starts. It is called on the process executing the function
        <code>Mod:Func</code>. Typical use of this function can be to alter
        the input parameters to the test case function (<code>Args</code>) or
        to set properties for the executing process.</p>
	<p>By returning <code>{skip,Reason}</code>, <code>Func</code> gets skipped.
	<code>Func</code> also gets skipped if <code>{auto_skip,Reason}</code> is returned,
	but then gets an auto skipped status (rather than user skipped).</p>
	<p>To fail <code>Func</code> immediately instead of executing it, return 
	<code>{fail,ReasonToFail}.</code></p>
      </div>
    </div>
    <div class="function">
      <h3 id="end_tc/3">end_tc(Mod,Func,Status) -&gt; ok | {fail,ReasonToFail}</h3>
      
      <ul class="type">
        <li><code>Mod = atom()</code></li>
	<d>Test suite name.</d>
        <li><code>Func = atom()</code></li>
	<d>Name of test case or configuration function.</d>
        <li><code>Status = {Result,Args} | {TCPid,Result,Args}</code></li>
	<d>The status of the test case or configuration function.</d>
	<li><code>ReasonToFail = term()</code></li>
	<d>Reason to fail the test case or configuration function.</d>
	<li><code>Result = ok | Skip | Fail</code></li>
	<d>The final result of the test case or configuration function.</d>
	<li><code>TCPid = pid()</code></li>
	<d>Pid of the process executing Func</d>
	<li><code>Skip = {skip,SkipReason}</code></li>
	<li><code>SkipReason = term() | {failed,{Mod,init_per_testcase,term()}}</code></li>
	<d>Reason why the function was skipped.</d>
	<li><code>Fail = {error,term()} | {'EXIT',term()} | {timetrap_timeout,integer()} |
	          {testcase_aborted,term()} | testcase_aborted_or_killed | 
		  {failed,term()} | {failed,{Mod,end_per_testcase,term()}}</code></li>
	<d>Reason why the function failed.</d>
        <li><code>Args = [tuple()]</code></li>
	<d>Normally Args = [Config]</d>
      </ul>
      <div class="description">
        <p>This function is called when a test case, or a configuration function,
	  is finished. It is normally called on the process where the function
          <code>Mod:Func</code> has been executing, but if not, the pid of the test 
	  case process is passed with the <code>Status</code> argument.</p> 
	  <p>Typical use of the <code>end_tc/3</code> function can be to clean up 
	  after <code>init_tc/3</code>.</p> 
	  <p>If <code>Func</code> is a test case, it is possible to analyse the value of 
	  <code>Result</code> to verify that <code>init_per_testcase/2</code> and 
	  <code>end_per_testcase/2</code> executed successfully.</p>
	  <p>It is possible with <code>end_tc/3</code> to fail an otherwise successful
	  test case, by returning <code>{fail,ReasonToFail}</code>. The test case <code>Func</code>
	  will be logged as failed with the provided term as reason.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="report/2">report(What,Data) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>What = atom()</code></li>
        <li><code>Data = term()</code></li>
      </ul>
      <div class="description">
        <p>This function is called in order to keep the framework up-to-date with 
	the progress of the test. This is useful e.g. if the
        framework implements a GUI where the progress information is
        constantly updated. The following can be reported:
        </p>
        <p><code>What = tests_start, Data = {Name,NumCases}</code><br />
	   <code>What = loginfo, Data = [{topdir,TestRootDir},{rundir,CurrLogDir}]</code><br />
	   <code>What = tests_done, Data = {Ok,Failed,{UserSkipped,AutoSkipped}}</code><br />
	   <code>What = tc_start, Data = {{Mod,{Func,GroupName}},TCLogFile}</code><br />
	   <code>What = tc_done, Data = {Mod,{Func,GroupName},Result}</code><br />
	   <code>What = tc_user_skip, Data = {Mod,{Func,GroupName},Comment}</code><br />
	   <code>What = tc_auto_skip, Data = {Mod,{Func,GroupName},Comment}</code><br />
	   <code>What = framework_error, Data = {{FWMod,FWFunc},Error}</code></p>
	<p>Note that for a test case function that doesn't belong to a group,
	  <code>GroupName</code> has value <code>undefined</code>, otherwise the name of the test
	  case group.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="error_notification/4">error_notification(Mod, Func, Args, Error) -&gt; ok</h3>
      
      <ul class="type">
        <li><code>Mod = atom()</code></li>
	<d>Test suite name.</d>
        <li><code>Func = atom()</code></li>
	<d>Name of test case or configuration function.</d>
        <li><code>Args = [tuple()]</code></li>
	<d>Normally Args = [Config]</d>
        <li><code>Error = {Reason,Location}</code></li>
	<li><code>Reason = term()</code></li>
	<d>Reason for termination.</d>
	<li><code>Location = unknown | [{Mod,Func,Line}]</code></li>
	<d>Last known position in Mod before termination.</d>
        <li><code>Line = integer()</code></li>
	<d>Line number in file Mod.erl.</d> 
      </ul>
      <div class="description">
        <p>This function is called as the result of function <code>Mod:Func</code> failing
	  with Reason at Location. The function is intended mainly to aid
	  specific logging or error handling in the framework application. Note
	  that for Location to have relevant values (i.e. other than unknown), 
	  the <code>line</code> macro or <code>test_server_line</code> parse transform must 
	  be used. For details, please see the section about test suite line numbers
	  in the <code>test_server</code> reference manual page.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="warn/1">warn(What) -&gt; boolean()</h3>
      
      <ul class="type">
        <li><code>What = processes | nodes</code></li>
      </ul>
      <div class="description">
        <p>The test server checks the number of processes and nodes
          before and after the test is executed. This function is a
          question to the framework if the test server should warn when
          the number of processes or nodes has changed during the test
          execution. If <code>true</code> is returned, a warning will be written
          in the test case minor log file.</p>
      </div>
    </div>
    <div class="function">
      <h3 id="target_info/0">target_info() -&gt; InfoStr</h3>
      
      <ul class="type">
        <li><code>InfoStr = string() | ""</code></li>
      </ul>
      <div class="description">
        <p>The test server will ask the framework for information about
	  the test target system and print InfoStr in the test case
	  log file below the host information.</p>
      </div>
    </div>
  </div>

      </div>
  </div>

    <script type="text/javascript">
      var CURRENT_ROOT = "../";
    </script>

    <script type="text/javascript" src="../jquery.js"></script>
    <script type="text/javascript" src="../erldocs_index.js"></script>
    <script type="text/javascript" src="../erldocs.js"></script>
  </body>
</html>
